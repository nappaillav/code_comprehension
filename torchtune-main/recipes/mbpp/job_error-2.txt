
Due to MODULEPATH changes, the following have been reloaded:
  1) arrow/14.0.1

[2024-09-03 15:04:56,329] torch.distributed.run: [WARNING] 
[2024-09-03 15:04:56,329] torch.distributed.run: [WARNING] *****************************************
[2024-09-03 15:04:56,329] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-03 15:04:56,329] torch.distributed.run: [WARNING] *****************************************
INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeDistributed with resolved config:

batch_size: 2
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: /home/vdhee/scratch/LLMcode/Train/models/
  checkpoint_files:
  - model-00001-of-00004.safetensors
  - model-00002-of-00004.safetensors
  - model-00003-of-00004.safetensors
  - model-00004-of-00004.safetensors
  model_type: LLAMA3
  output_dir: /home/vdhee/scratch/LLMcode/Train/models/output
  recipe_checkpoint: null
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
device: cuda
dtype: bf16
enable_activation_checkpointing: false
epochs: 1
gradient_accumulation_steps: 32
log_every_n_steps: 1
log_peak_memory_stats: false
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
lr_scheduler:
  _component_: torchtune.modules.get_cosine_schedule_with_warmup
  num_warmup_steps: 100
max_steps_per_epoch: null
metric_logger:
  _component_: torchtune.utils.metric_logging.DiskLogger
  log_dir: /tmp/lora_finetune_output
model:
  _component_: torchtune.models.llama3_1.lora_llama3_1_8b
  apply_lora_to_mlp: false
  apply_lora_to_output: false
  lora_alpha: 16
  lora_attn_modules:
  - q_proj
  - v_proj
  lora_rank: 8
optimizer:
  _component_: torch.optim.AdamW
  lr: 0.0003
  weight_decay: 0.01
output_dir: /tmp/lora_finetune_output
resume_from_checkpoint: false
save_adapter_weights_only: false
seed: null
shuffle: true
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: null
  path: /home/vdhee/scratch/LLMcode/Train/models/original/tokenizer.model

DEBUG:torchtune.utils.logging:Setting manual seed to local seed 4120044885. Local seed is seed + rank = 4120044885 + 0
INFO:torchtune.utils.logging:FSDP is enabled. Instantiating Model on CPU for Rank 0 ...
INFO:torchtune.utils.logging:Model instantiation took 17.36 secs
INFO:torchtune.utils.logging:Memory stats after model init:
	GPU peak memory allocation: 11.40 GiB
	GPU peak memory reserved: 12.49 GiB
	GPU peak memory active: 11.40 GiB
INFO:torchtune.utils.logging:Optimizer and loss are initialized.
INFO:torchtune.utils.logging:Loss is initialized.
INFO:torchtune.utils.logging:Loss is initialized.
INFO:torchtune.utils.logging:Learning rate scheduler is initialized.
WARNING:torchtune.utils.logging: Profiling disabled.
INFO:torchtune.utils.logging: Profiler config after instantiation: {'enabled': False}
  0%|          | 0/2 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/recipes/lora_finetune_distributed.py", line 1267, in <module>
    sys.exit(recipe_main())
             ^^^^^^^^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/config/_parse.py", line 99, in wrapper
    sys.exit(recipe_main(conf))
             ^^^^^^^^^^^^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/recipes/lora_finetune_distributed.py", line 1262, in recipe_main
    recipe.train()
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/recipes/lora_finetune_distributed.py", line 1139, in train
    logits_a2 = self._model(input_ids_a2, mask=mask, input_pos=None)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 849, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/modules/transformer.py", line 458, in forward
    h = layer(
        ^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 849, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/modules/transformer.py", line 105, in forward
    mlp_out = self.mlp(self.mlp_norm(h))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/modules/feed_forward.py", line 37, in forward
    return self.w2(self.activation(self.w1(x)) * self.w3(x))
                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 124.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 36.12 MiB is free. Including non-PyTorch memory, this process has 39.34 GiB memory in use. Of the allocated memory 37.51 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  0%|          | 0/2 [00:03<?, ?it/s]
[2024-09-03 15:09:31,355] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1625689 closing signal SIGTERM
[2024-09-03 15:09:31,869] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1625688) of binary: /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.11.5/bin/python
Traceback (most recent call last):
  File "/home/vdhee/.local/bin/tune", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/_cli/tune.py", line 49, in main
    parser.run(args)
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/_cli/tune.py", line 43, in run
    args.func(args)
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/_cli/run.py", line 177, in _run_cmd
    self._run_distributed(args)
  File "/lustre07/scratch/vdhee/LLMcode/Train/torchtune/torchtune/_cli/run.py", line 88, in _run_distributed
    run(args)
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vdhee/.local/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/lustre07/scratch/vdhee/LLMcode/Train/torchtune/recipes/lora_finetune_distributed.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-03_15:09:31
  host      : ng20204.narval.calcul.quebec
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1625688)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
